{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stationarity - Intuition and Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I can toss a coin 4 times, and write the outcomes down on a piece of paper. Perheps I would get heads, tails, tails, tails. That's a time series. It's a set of observations.\n",
    "\n",
    "We can also try to model that situation mathematically by lining up four Bernolli random variables (Bernoulli random variable has a sucess or a failure and a probability).\n",
    "\n",
    "So we have these 4 random variables, We could also talk about how they're related to each other. For the coin toss I would imagine that they are all independent.\n",
    "\n",
    "So we're making a statement about the structure as well, how the random variables are related to each other.\n",
    "\n",
    "Stochastic process will do the same thing. We will look at a set of random variables, maybe there are 4 of them, maybe there are a million, maybe there's a countable infinity, maybe an uncountable infinity, but for each one of the random variables along our process we've indexed let's say with time.\n",
    "\n",
    "For each of the random variables along our process, we understand the nature of the random variable but we also understand how the random varibles are related to onde another.\n",
    "\n",
    "We can also have a continuous process. So we're looking at the index now and discussing the index is either discrete of continuous.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph on the left looks a like a total mess. ![Alt text](https://uploaddeimagens.com.br/images/001/921/639/full/Capturar.PNG?1551008196)\n",
    "\n",
    "Look at the graph on the right, we have random walks with Gaussian increments.\n",
    "The idea here is that we park outselves at some initial position and then we'll move to the left or the right randomly, and the random variable guiding our motion, in this case, is a Gaussian random variable.\n",
    "\n",
    "But I could just as easily have had a coin toss and moved to the right or the left by one step depending upon whether I got heads or tails.\n",
    "\n",
    "This is a little bit more complicated in that our times or that our step size is determined off a Gaussian distribuition.\n",
    "\n",
    "We obtained these 4 different realizations, we are thinking of these as 4 differents time series, but all those 4 time series all come from the same stochastic process.\n",
    "\n",
    "The reason nthe graph on the left looks like sucha a mess it that there's no real structure going on at all. I'm thinking about a simple random sample. The kind of situation you would have dealt with in elementary statistics class, where the random variables are independent of one another.\n",
    "\n",
    "So let's say you are measuring temperatures, so the temperature you get from the fifth person is totally independent of the temperature taht you's get from the 60th or up to the 1.000th in this case.\n",
    "\n",
    "The inner variables are independent, they are identicallly distributed and so no real structure. And so when I graph the 4 in the same axis there's just as we said a total mess."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensembles and Realizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stochastic process really is a very complicated mathematical thing.\n",
    "\n",
    "In your elements or stats course you might have dealt with bivariate normal distributions and I could, perhaps, do the integrations by hand in some very simple cases. But a stochastic process isn't necessarily 2 or 3 or 20 random variables, you might have an infinite number of random variables.\n",
    "\n",
    "To fully specify the structure there, you need the <b>joint distribution of the full set of random variables</b>. That could be very difficult to work with.\n",
    "\n",
    "We also usually just have a set of data, a time series. Some data that we have gone out and acquired, and we'll try to understand the properties of the stochastic process off of this particular time series\n",
    "\n",
    "How can we do that sort of inference?\n",
    "\n",
    "As we get started, and we'll introduce stationaruty in just a moment, we should reviwe the concepts of mean function and variance function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean, Variance and Autocovariance Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since your stochastic process is an indexed set of random variables, let's assume for the moment that each one of those random variables has a means and a variance\n",
    "\n",
    "We can use that to create a mean function, so as I move along the stochastic process I observe what the average is for the random varibale at any individual time.\n",
    "\n",
    "Mean Function:\n",
    "$$\n",
    "\\mu(t) \\equiv \\mu_t \\equiv E[X(t)]\n",
    "$$\n",
    "\n",
    "Variance Function:\n",
    "$$\n",
    "\\sigma^2(t) \\equiv \\sigma_t^2 \\equiv V[X(t)]\n",
    "$$\n",
    "\n",
    "In this little table we are implicitly assuming we have a discrete stochastic process, and we're writing out the expected value in the variance as we move thought for each of our random variables, and we can do a graph or plot of mean as a function of index, or variant as a function of index.\n",
    "\n",
    "![table](https://uploaddeimagens.com.br/images/001/921/657/full/Capturar.PNG?1551011227)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also talk about the relationship. Let's look at a very simple case of white noise.\n",
    "\n",
    "Mean Function: $$\\mu(t) = $$\n",
    "\n",
    "Variance Function:$$\\sigma^2(t) =$$\n",
    "\n",
    "AutoCovariance Function:$$\\gamma(t_1,t_2) =$$\n",
    "\n",
    "\n",
    "The mean funciotn there $\\mu(t)$, tou can see that would be a contant function.\n",
    "\n",
    "So when we have white noise, we have independent identically distributed random variables.\n",
    "\n",
    "If they're identically distributed, then the mean function, which is be constant as we move along.\n",
    "\n",
    "We're trying to summarize the random variables at different time locations with same mean and variance\n",
    "\n",
    "If you have independent identically distributed random variables the other covariance function will look like a delta function.\n",
    "\n",
    "Our autocovariance function when the random variables, the separation is zero, we're going to be just look at the variance we'll get a head.\n",
    "\n",
    "But as soon as we separate the 2 random variables and look at 2 different times, since we're independent, the covariance will be zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can we infer the properties of a stochastic process from a single realization?\n",
    "\n",
    "Since you don't have typically many realizations in front of you. You just have one realization.\n",
    "\n",
    "Think about one of the trajectories with that random walk, how are you going to infer the properties of the process?\n",
    "\n",
    "Each one of the random variables along your time series is only giving you an individual point.\n",
    "\n",
    "So if I have a population and I just have a sample size n = 1, I really can't say anything about the variance and I can say very meager things about the mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Strict Stationarity: Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we introduce some structure, we can get some traction.\n",
    "\n",
    "So let's talk about a process and say that it's strict stationary if the joint distribution of a set of random variables:\n",
    "\n",
    "$$\n",
    "X(t_1), X(t_2), ...... X(t_k)\n",
    "$$\n",
    "\n",
    "Will be the same no matter where you look along the time series as long as each one of the new random variables is just a shiffed copy of the old random variables.\n",
    "\n",
    "So park your self anywhere you'd like along the time series and look at the set of random variables.\n",
    "\n",
    "Preserve the spacing between them, but now look for to the left or for to the right along the stochastic process.\n",
    "\n",
    "You will get the same joint distribution if your process is strictly stationary.\n",
    "\n",
    "$$\n",
    "X(t_1 + \\tau), X(t_2 + \\tau), ...... X(t_k+ \\tau)\n",
    "$$\n",
    "\n",
    "\n",
    "It's a very restrictive thing to say."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Strict Stationarity: Implications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But it has some implications that work well for us.\n",
    "\n",
    "If you are strictly stationary then if we only look at one of them, lets say k = 1, the distribution on any reanom variable along our stochastic process is the same as the distribution shifted by whatever amount we like\n",
    "\n",
    "Implication:\n",
    "\n",
    "Distribution of $X(t_1)$ same as distribution of $X(t_1 + \\tau)$\n",
    "\n",
    "What that means is the random variables are identically distributed.\n",
    "\n",
    "They might not be independent and in fact if they're indentically distributed and independent then wen don't have a very interesting process at all.\n",
    "\n",
    "Mean Function: $\\mu(t) = \\mu$\n",
    "\n",
    "Variance Function: $\\sigma^2(t) = \\sigma^2$\n",
    "\n",
    "The mean function though, since we get identically distributed random variables will be a constant and the variance function will be a constant.\n",
    "\n",
    "That has implications for estimation.\n",
    "\n",
    "We can use each one of the data points we have available to us in the time series to try to estimate the mean of the process. Same thing with the variance\n",
    "\n",
    "What are the implications fot the Autocovariance:\n",
    "\n",
    "Implication:\n",
    "\n",
    "Joint Distribution of $X(t_1), X(t_2)$ same as joint distribution of $X(t_1 + \\tau), X(t_2 + \\tau)$\n",
    "\n",
    "If we look at joint distribuition of 2 rated variables, t1 and t2,  that'll be the same as th joint distribution if I lokk up or down on the stochastic process.\n",
    "\n",
    "So if I shift to the left or the right by distance $\\tau$ \n",
    "\n",
    "What that's telling us is that the joint distribution of 2 random variables depends only on the lag spacing and not where you are on the random process\n",
    "\n",
    "Autocovariance Function: $\\gamma(t_1,t_2) = \\gamma(t_2 - t_1) = \\gamma(\\tau)$\n",
    "\n",
    "So our autocovariance function isn't constant, but the autocovariance just depends upon there's a separation between the 2 random variables.\n",
    "\n",
    "No matter where you look, to the left or the right on the distribution, autocovariance only depends upon separation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weak Stationarity: Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Strict stationarity dos a lot of work for us but it's a pretty restrictive concept.\n",
    "\n",
    "We can get the same sort of things done for us 'relax' a little but, and view weak stationarity.\n",
    "\n",
    "So process is weakly stationary if we keep all of the things that we really care about from a strictly stationary process.\n",
    "\n",
    "A process is weakly stationary if the mean function depends not on where you look along the process but rather is constant.\n",
    "\n",
    "Mean Function: $\\mu(t) = \\mu$\n",
    "\n",
    "So we have constant average up and down the process\n",
    "\n",
    "We will also say weakly stationary if the autocovariance function just depends upon lag spacing.\n",
    "\n",
    "ACF: $\\gamma(t_1,t_2) = \\gamma(t_2 - t_1) = \\gamma(\\tau)$\n",
    "\n",
    "So implication from strict stationarity we're using within the definition of weak stationarity, keeping what we want.\n",
    "\n",
    "Of course, if your autocovariance function just depends upon lag spacing then length of lag equals zero, and get immediatly that it's a constant variance function as well.\n",
    "\n",
    "Much easier to think about, much easier to state but still very useful for us."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Time_Series_Analysis-Cousera]",
   "language": "python",
   "name": "conda-env-Time_Series_Analysis-Cousera-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
